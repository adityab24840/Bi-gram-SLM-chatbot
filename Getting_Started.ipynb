{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/black-forest-labs/FLUX.1-dev|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Tenserflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline() is the easiest and fastest way to use a pretrained model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/en/main_classes/pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --force-reinstall torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic of the code: ChatBot using a Simple/Small Language Model (SLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk import bigrams, FreqDist, ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If everything went fine, Reuters articles are saved as text files.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "# Ensure that the NLTK Reuters corpus is downloaded\n",
    "nltk.download('reuters')\n",
    "\n",
    "# Create a directory to store the text files\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Iterate through the fileids in the Reuters corpus\n",
    "for fileid in reuters.fileids():\n",
    "    # Retrieve the text of the news article\n",
    "    article_text = ' '.join(reuters.words(fileid))\n",
    "\n",
    "    # Construct the filename (you can also use\n",
    "    # the categories in the filename if needed)\n",
    "    filename = f'data/{fileid.replace(\"/\", \"_\")}.txt'\n",
    "\n",
    "    # Write the article text to a file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(article_text)\n",
    "        \n",
    "print(\"If everything went fine, Reuters articles are saved as text files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_dir = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String of punctuation without the full stop\n",
    "punctuation = string.punctuation.replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hidden(filepath):\n",
    "    return os.path.basename(filepath).startswith('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data=\"\"\n",
    "for filename in os.listdir(input_data_dir):\n",
    "    filepath = os.path.join(input_data_dir, filename)\n",
    "    if not is_hidden(filepath):\n",
    "        with open(filepath) as infile:\n",
    "            for line in infile:\n",
    "                if line.strip():  # Check if line is not just whitespace\n",
    "                    # Remove all punctuation except full stops\n",
    "                    for char in punctuation:\n",
    "                        line = line.replace(char, '')\n",
    "                    text_data += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8441834"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab')\n",
    "words = nltk.word_tokenize(text_data.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bigrams\n",
    "bi_grams = list(bigrams(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 391847 samples and 1564778 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# Calculate frequency distribution for each bigram\n",
    "bi_gram_freq_dist = FreqDist(bi_grams)\n",
    "print(bi_gram_freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('asian', 'exporters'), 1)\n",
      "(('exporters', 'fear'), 2)\n",
      "(('fear', 'damage'), 1)\n",
      "(('damage', 'from'), 8)\n",
      "(('from', 'u'), 27)\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "# Print the first five elements of the dictionary\n",
    "first_five_items = list(islice(bi_gram_freq_dist.items(), 5))\n",
    "for item in first_five_items:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute conditional frequency distribution of bigrams\n",
    "bi_gram_freq = ConditionalFreqDist(bi_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'gas': 216, 'rubber': 39, 'resources': 9, 'for': 3, 'float': 3, 'disasters': 2, 'that': 2, 'lt': 2, 'lower': 1, 'beverages': 1, ...})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_freq['natural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'situation': 2, 'barriers': 1, 'sweeteners': 1, 'intelligence': 1, '.': 1, 'controls': 1, 'pace': 1, 'hearts': 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_freq['artificial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "topk=3\n",
    "# Create a dictionary to hold the top topk bigrams for each first word\n",
    "top_bigrams_per_first_word = {}\n",
    "\n",
    "# Iterate over the bigram frequency distribution\n",
    "for (first_word, second_word), freq in bi_gram_freq_dist.items():\n",
    "    # Initialize an empty heap for the first_word if it doesn't exist\n",
    "    if first_word not in top_bigrams_per_first_word:\n",
    "        top_bigrams_per_first_word[first_word] = []\n",
    "\n",
    "    # Add to the heap and maintain top topk\n",
    "    heapq.heappush(top_bigrams_per_first_word[first_word],\n",
    "                   (freq, second_word))\n",
    "    if len(top_bigrams_per_first_word[first_word]) > topk:\n",
    "        heapq.heappop(top_bigrams_per_first_word[first_word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 'resources'), (216, 'gas'), (39, 'rubber')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_bigrams_per_first_word['natural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'pace'), (1, 'sweeteners'), (2, 'situation')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_bigrams_per_first_word['artificial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the heap to a simple list for each first word\n",
    "for first_word in top_bigrams_per_first_word:\n",
    "    sorted_bigrams = sorted(\n",
    "        top_bigrams_per_first_word[first_word], reverse=True)\n",
    "    top_bigrams_list = []\n",
    "    for freq, second_word in sorted_bigrams:\n",
    "        top_bigrams_list.append(second_word)\n",
    "    top_bigrams_per_first_word[first_word] = top_bigrams_list\n",
    "\n",
    "# Use these filtered bigrams to create a ConditionalFreqDist\n",
    "filtered_bi_grams = []\n",
    "for first_word in top_bigrams_per_first_word:\n",
    "    for second_word in top_bigrams_per_first_word[first_word]:\n",
    "        filtered_bi_grams.append((first_word, second_word))\n",
    "\n",
    "bi_gram_freq = ConditionalFreqDist(filtered_bi_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(word, num_words):\n",
    "    word =word.lower()\n",
    "    for _ in range(num_words):\n",
    "        print(word, end=' ')\n",
    "        next_words = [item for item, freq in bi_gram_freq[word].items()]\n",
    "        if len(next_words) > 0:\n",
    "            # Randomly choose a next word\n",
    "            word = random.choice(next_words)\n",
    "        else:\n",
    "            break  # Break if the word has no following words\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asia and a new company said . s lt xon unit of its stake to be a year . the u . s stock . the dollar . s stock split . s stock split . the u . 5 mln stg in 1986 net profit of the u trans world s . s stock . the company said it is expected to be made in a year . 5 pct in 1986 net profit 2 mln dlrs . the company said . the dollar . the dollar s lt c nil net profit 2 mln stg in 1986 . \n"
     ]
    }
   ],
   "source": [
    "generate_sentence('Asia', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank of the dollar . the u . s lt \n"
     ]
    }
   ],
   "source": [
    "generate_sentence('bank', 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
